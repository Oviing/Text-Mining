{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models used in this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Used for task 1\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "#used for task 2\n",
    "from nltk import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#used in part B task 1\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, cross_validate, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#used in part B task 1 unsupervised\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#used in part B task 2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first task of the assignment we are using a drug review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/joel/Desktop/drugLib_raw/drugLibTrain_raw.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>urlDrugName</th>\n",
       "      <th>rating</th>\n",
       "      <th>effectiveness</th>\n",
       "      <th>sideEffects</th>\n",
       "      <th>condition</th>\n",
       "      <th>benefitsReview</th>\n",
       "      <th>sideEffectsReview</th>\n",
       "      <th>commentsReview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2202</td>\n",
       "      <td>enalapril</td>\n",
       "      <td>4</td>\n",
       "      <td>Highly Effective</td>\n",
       "      <td>Mild Side Effects</td>\n",
       "      <td>management of congestive heart failure</td>\n",
       "      <td>slowed the progression of left ventricular dys...</td>\n",
       "      <td>cough, hypotension , proteinuria, impotence , ...</td>\n",
       "      <td>monitor blood pressure , weight and asses for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3117</td>\n",
       "      <td>ortho-tri-cyclen</td>\n",
       "      <td>1</td>\n",
       "      <td>Highly Effective</td>\n",
       "      <td>Severe Side Effects</td>\n",
       "      <td>birth prevention</td>\n",
       "      <td>Although this type of birth control has more c...</td>\n",
       "      <td>Heavy Cycle, Cramps, Hot Flashes, Fatigue, Lon...</td>\n",
       "      <td>I Hate This Birth Control, I Would Not Suggest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1146</td>\n",
       "      <td>ponstel</td>\n",
       "      <td>10</td>\n",
       "      <td>Highly Effective</td>\n",
       "      <td>No Side Effects</td>\n",
       "      <td>menstrual cramps</td>\n",
       "      <td>I was used to having cramps so badly that they...</td>\n",
       "      <td>Heavier bleeding and clotting than normal.</td>\n",
       "      <td>I took 2 pills at the onset of my menstrual cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3947</td>\n",
       "      <td>prilosec</td>\n",
       "      <td>3</td>\n",
       "      <td>Marginally Effective</td>\n",
       "      <td>Mild Side Effects</td>\n",
       "      <td>acid reflux</td>\n",
       "      <td>The acid reflux went away for a few months aft...</td>\n",
       "      <td>Constipation, dry mouth and some mild dizzines...</td>\n",
       "      <td>I was given Prilosec prescription at a dose of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1951</td>\n",
       "      <td>lyrica</td>\n",
       "      <td>2</td>\n",
       "      <td>Marginally Effective</td>\n",
       "      <td>Severe Side Effects</td>\n",
       "      <td>fibromyalgia</td>\n",
       "      <td>I think that the Lyrica was starting to help w...</td>\n",
       "      <td>I felt extremely drugged and dopey.  Could not...</td>\n",
       "      <td>See above</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       urlDrugName  rating         effectiveness  \\\n",
       "0        2202         enalapril       4      Highly Effective   \n",
       "1        3117  ortho-tri-cyclen       1      Highly Effective   \n",
       "2        1146           ponstel      10      Highly Effective   \n",
       "3        3947          prilosec       3  Marginally Effective   \n",
       "4        1951            lyrica       2  Marginally Effective   \n",
       "\n",
       "           sideEffects                               condition  \\\n",
       "0    Mild Side Effects  management of congestive heart failure   \n",
       "1  Severe Side Effects                        birth prevention   \n",
       "2      No Side Effects                        menstrual cramps   \n",
       "3    Mild Side Effects                             acid reflux   \n",
       "4  Severe Side Effects                            fibromyalgia   \n",
       "\n",
       "                                      benefitsReview  \\\n",
       "0  slowed the progression of left ventricular dys...   \n",
       "1  Although this type of birth control has more c...   \n",
       "2  I was used to having cramps so badly that they...   \n",
       "3  The acid reflux went away for a few months aft...   \n",
       "4  I think that the Lyrica was starting to help w...   \n",
       "\n",
       "                                   sideEffectsReview  \\\n",
       "0  cough, hypotension , proteinuria, impotence , ...   \n",
       "1  Heavy Cycle, Cramps, Hot Flashes, Fatigue, Lon...   \n",
       "2         Heavier bleeding and clotting than normal.   \n",
       "3  Constipation, dry mouth and some mild dizzines...   \n",
       "4  I felt extremely drugged and dopey.  Could not...   \n",
       "\n",
       "                                      commentsReview  \n",
       "0  monitor blood pressure , weight and asses for ...  \n",
       "1  I Hate This Birth Control, I Would Not Suggest...  \n",
       "2  I took 2 pills at the onset of my menstrual cr...  \n",
       "3  I was given Prilosec prescription at a dose of...  \n",
       "4                                          See above  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the following tasks we considere just the column \"SideEffectsReview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['sideEffectsReview']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Heavy Cycle, Cramps, Hot Flashes, Fatigue, Long Lasting Cycles. It's only been 5 1/2 months, but i'm concidering changing to a different bc. This is my first time using any kind of bc, unfortunately due to the constant hassel, i'm not happy with the results.\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see an output of a row in our dataset. Typically people write down their side effects and how much or how long they have taken a drug. We can see each single review as a document while X is then a collection of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Building a pipeline to remoce punctuation and stopwords. As well we want to tokenize the data. The result should be a list of bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a class to clean the text data\n",
    "# 1. A sentence or several sentences are tokenized in a list of separeted elements rowise\n",
    "# 2. For each list punctuations are removed\n",
    "# 3. returns a cleaned list\n",
    "class CleanText():\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def clean_punctuation(self, x):\n",
    "        \n",
    "        #returns a list with tokinized words. Each string is a seperate element of the list\n",
    "        x = str(x)\n",
    "        w = word_tokenize(x)\n",
    "        \n",
    "        #removes all non alphabetic characters\n",
    "        clean_list = [i for i in w if i.isalpha()]\n",
    "        \n",
    "        return clean_list\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        \n",
    "        l = []\n",
    "        \n",
    "\n",
    "        \n",
    "        for i in X:\n",
    "            \n",
    "            cp = self.clean_punctuation(i)\n",
    "            l.append(cp)\n",
    "        \n",
    "        \n",
    "        return l #X.apply(self.clean_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a class to remove stopwords\n",
    "\n",
    "class Stop():\n",
    "    \n",
    "    def __init__(self, language):\n",
    "        self.lang = language\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def remove_stop_words(self, x):\n",
    "        cleaned_list = [w for w in x if not w in stopwords.words(self.lang)]\n",
    "        return cleaned_list\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        l = []\n",
    "        for i in X:\n",
    "            cleaned_sublist = self.remove_stop_words(i)\n",
    "            l.append(cleaned_sublist)\n",
    "        \n",
    "        return l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a class to generate n-grams\n",
    "# returns a nested list where each sublist is a list for each row with n-grams\n",
    "class NGram():\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        ngram_total_list = []\n",
    "        for i in range(0, len(X)):\n",
    "            ngram = ngrams(X[i], n = self.n)\n",
    "            \n",
    "            ngram = list(ngram)\n",
    "            \n",
    "            ngram_total_list.append(ngram)\n",
    "            \n",
    "        return ngram_total_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"Remove_Punktuation\", CleanText()),\n",
    "                 (\"Stopwords\", Stop('english')),\n",
    "               (\"NGram\", NGram(2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_result = pipe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3107"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pipe_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Heavy', 'Cycle'),\n",
       " ('Cycle', 'Cramps'),\n",
       " ('Cramps', 'Hot'),\n",
       " ('Hot', 'Flashes'),\n",
       " ('Flashes', 'Fatigue'),\n",
       " ('Fatigue', 'Long'),\n",
       " ('Long', 'Lasting'),\n",
       " ('Lasting', 'Cycles'),\n",
       " ('Cycles', 'It'),\n",
       " ('It', 'months'),\n",
       " ('months', 'concidering'),\n",
       " ('concidering', 'changing'),\n",
       " ('changing', 'different'),\n",
       " ('different', 'bc'),\n",
       " ('bc', 'This'),\n",
       " ('This', 'first'),\n",
       " ('first', 'time'),\n",
       " ('time', 'using'),\n",
       " ('using', 'kind'),\n",
       " ('kind', 'bc'),\n",
       " ('bc', 'unfortunately'),\n",
       " ('unfortunately', 'due'),\n",
       " ('due', 'constant'),\n",
       " ('constant', 'hassel'),\n",
       " ('hassel', 'happy'),\n",
       " ('happy', 'results')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first Task we removed stopwords and tokenized the text. Then we built bigrams for each document. Above we see the result. We have a nested list where each individual list is a collection of bigrams for a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline([(\"Remove_Punktuation\", CleanText()),\n",
    "                 (\"Stopwords\", Stop('english'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_result2 = pipe2.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Heavy',\n",
       " 'Cycle',\n",
       " 'Cramps',\n",
       " 'Hot',\n",
       " 'Flashes',\n",
       " 'Fatigue',\n",
       " 'Long',\n",
       " 'Lasting',\n",
       " 'Cycles',\n",
       " 'It',\n",
       " 'months',\n",
       " 'concidering',\n",
       " 'changing',\n",
       " 'different',\n",
       " 'bc',\n",
       " 'This',\n",
       " 'first',\n",
       " 'time',\n",
       " 'using',\n",
       " 'kind',\n",
       " 'bc',\n",
       " 'unfortunately',\n",
       " 'due',\n",
       " 'constant',\n",
       " 'hassel',\n",
       " 'happy',\n",
       " 'results']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_result2[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Split the corpus into sentences and vectorize it into a bag of words and TF-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Heavy Cycle, Cramps, Hot Flashes, Fatigue, Long Lasting Cycles. It's only been 5 1/2 months, but i'm concidering changing to a different bc. This is my first time using any kind of bc, unfortunately due to the constant hassel, i'm not happy with the results.\""
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a class to create sentences out of the text\n",
    "# the function is applied rowise on the input dataframe\n",
    "class text2sentence():\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def sentence(self, x):\n",
    "        x = str(x)\n",
    "        s = sent_tokenize(x)\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        \n",
    "        #X_sentence = X.apply(self.sentence)\n",
    "        X_sentence = [self.sentence(w) for w in X]\n",
    "        return X_sentence #list(X_sentence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer(analyzer='word', lowercase = True)\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        #bow = self.vectorizer.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "\n",
    "        bow_transform = self.vectorizer.fit_transform(X)\n",
    "        names_of_vectors = self.vectorizer.get_feature_names()\n",
    "        \n",
    "        return bow_transform, names_of_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a class to flatten a nested list into a list\n",
    "# This is necessary if we split our data into sentences befor applying the bag of words algorithm on the data\n",
    "class Flatten():\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        \n",
    "        flatten = [item for sublist in X for item in sublist]\n",
    "        \n",
    "        return flatten   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' a class to lower case capitalized words. As well this class merges sub lists to a string\n",
    " Used before applying bag of Words or TF-ID '''\n",
    "\n",
    "class Lower():\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        \n",
    "        lower_list = []\n",
    "        for i in X:\n",
    "            X_actual = i\n",
    "            lower = [x.lower() for x in X_actual]\n",
    "            lower_list.append(lower)\n",
    "        \n",
    "        list_of_list_to_string = [''.join(l) for l in lower_list]\n",
    "                          \n",
    "        return list_of_list_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_ID():\n",
    "    \n",
    "    def __init__(self, explainable):\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='word')\n",
    "        self.explain = explainable\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        #bow = self.vectorizer.fit(X)\n",
    "        return self.vectorizer.fit(X)\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        \n",
    "        if self.explain == True:\n",
    "            \n",
    "            tfid_transform = self.vectorizer.transform(X)\n",
    "            names_of_vectors = self.vectorizer.get_feature_names()\n",
    "        \n",
    "            return tfid_transform, names_of_vectors\n",
    "        \n",
    "        else:\n",
    "            tfid_transform = self.vectorizer.transform(X)\n",
    "            return tfid_transform.toarray()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_1 = Pipeline([\n",
    "                (\"Sentences\", text2sentence()),\n",
    "                 (\"Flatten\", Flatten()),\n",
    "                 (\"BoW\", BagOfWords())])\n",
    "\n",
    "#we applied a flatten function to merge sublist into a list. \n",
    "#We need to do this to allow to apply BoW on the complete corpus and not just one a singe document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_result_1, names_bow = pipe_1.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000mg</th>\n",
       "      <th>025</th>\n",
       "      <th>05</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>...</th>\n",
       "      <th>zithromycin</th>\n",
       "      <th>zofran</th>\n",
       "      <th>zoloft</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombing</th>\n",
       "      <th>zomig</th>\n",
       "      <th>zone</th>\n",
       "      <th>zyban</th>\n",
       "      <th>zyprexa</th>\n",
       "      <th>zyrtec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6878 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  000mg  025  05  07  08  10  100  1000  ...  zithromycin  zofran  \\\n",
       "0   0    0      0    0   0   0   0   0    0     0  ...            0       0   \n",
       "1   0    0      0    0   0   0   0   0    0     0  ...            0       0   \n",
       "2   0    0      0    0   0   0   0   0    0     0  ...            0       0   \n",
       "3   0    0      0    0   0   0   0   0    0     0  ...            0       0   \n",
       "4   0    0      0    0   0   0   0   0    0     0  ...            0       0   \n",
       "\n",
       "   zoloft  zombie  zombing  zomig  zone  zyban  zyprexa  zyrtec  \n",
       "0       0       0        0      0     0      0        0       0  \n",
       "1       0       0        0      0     0      0        0       0  \n",
       "2       0       0        0      0     0      0        0       0  \n",
       "3       0       0        0      0     0      0        0       0  \n",
       "4       0       0        0      0     0      0        0       0  \n",
       "\n",
       "[5 rows x 6878 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix = pd.DataFrame(pipe_result_1.toarray(), columns = names_bow)\n",
    "bow_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better demonstration we built a class for bag of words, which also returns us the names. With this we can build a dataframe which shows in for each row a document and the each column counts the appeareance of the word. As we see the result is a very sparse matrix. Also we notice that we just splitted the original corpus into sentences and applied directly a a bag of words algorithm on it. As result we also have strings / words as 00, 000mg and other values. For later tasks we may will clean this as well, but in this exercise it would be out of scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_2 = Pipeline([(\"Remove_Punktuation\", CleanText()),\n",
    "                 (\"Stopwords\", Stop('english')),\n",
    "                 (\"Sentences\", text2sentence()),\n",
    "                 (\"Flatten\", Flatten()),\n",
    "                 (\"TFID\", TF_ID(explainable = True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-225-6768d5acf410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipe_result_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "pipe_result_2, names = pipe_2.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we splitted the corpus into sentences we applied a flatten function on our list. The reason therefor is that the splitting resulted into a list of nested list which we transform through flatten into one list. On this we then apply the Bag of Words or TF-ID Method. Both pipelines output a very sparse matrix.\n",
    "\n",
    "We applied in our pipeline as well the classes for removing punctuations and stepwords. In the original data we are also confronted with numeric values as 10mg, 1-2 days, ect. This are informations which are not directly useful for us and just increase the dimension of our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_term_matrix = pd.DataFrame(pipe_result_2.toarray(), columns = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abated</th>\n",
       "      <th>abbsessed</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abdominal</th>\n",
       "      <th>abfter</th>\n",
       "      <th>abilify</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>...</th>\n",
       "      <th>zithromycin</th>\n",
       "      <th>zofran</th>\n",
       "      <th>zoloft</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombing</th>\n",
       "      <th>zomig</th>\n",
       "      <th>zone</th>\n",
       "      <th>zyban</th>\n",
       "      <th>zyprexa</th>\n",
       "      <th>zyrtec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6525 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abandon  abandoning  abated  abbsessed  abdomen  abdominal  abfter  \\\n",
       "0      0.0         0.0     0.0        0.0      0.0        0.0     0.0   \n",
       "1      0.0         0.0     0.0        0.0      0.0        0.0     0.0   \n",
       "2      0.0         0.0     0.0        0.0      0.0        0.0     0.0   \n",
       "3      0.0         0.0     0.0        0.0      0.0        0.0     0.0   \n",
       "4      0.0         0.0     0.0        0.0      0.0        0.0     0.0   \n",
       "\n",
       "   abilify  abilities  ability  ...  zithromycin  zofran  zoloft  zombie  \\\n",
       "0      0.0        0.0      0.0  ...          0.0     0.0     0.0     0.0   \n",
       "1      0.0        0.0      0.0  ...          0.0     0.0     0.0     0.0   \n",
       "2      0.0        0.0      0.0  ...          0.0     0.0     0.0     0.0   \n",
       "3      0.0        0.0      0.0  ...          0.0     0.0     0.0     0.0   \n",
       "4      0.0        0.0      0.0  ...          0.0     0.0     0.0     0.0   \n",
       "\n",
       "   zombing  zomig  zone  zyban  zyprexa  zyrtec  \n",
       "0      0.0    0.0   0.0    0.0      0.0     0.0  \n",
       "1      0.0    0.0   0.0    0.0      0.0     0.0  \n",
       "2      0.0    0.0   0.0    0.0      0.0     0.0  \n",
       "3      0.0    0.0   0.0    0.0      0.0     0.0  \n",
       "4      0.0    0.0   0.0    0.0      0.0     0.0  \n",
       "\n",
       "[5 rows x 6525 columns]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better visualization of the result we constructed a data frame out of the TF-ID matrix. As we see above we are dealing with a very sparse matrix where most of the elements are zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B\n",
    "\n",
    "For the second part of the assignment we not directly use the same pipeline as we consotructed before. We will skip some steps and add some more. In detail we will skip the step splitting long strings into sentences. We have to remember that each row is a document and for each document we have predefined label. If we would split inside our documents the text into sentences we would have for each document a several lists.\n",
    "\n",
    "For part B we not apply use the before constructed pipelines. We will skip to steps from the previous one. We not split up our corpus into sentences and therefor we also not apply the flatten function on the resulting list. We do this because spliting our data into sentences will increase lenght of our total list. This results in the problem that our labels from before do not fit anymore. We have more datapoints than labels and we not know the relationship between each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(word_tokenize(X[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Heavy', 'Cycle', ',', 'Cramps', ',', 'Hot', 'Flashes', ',',\n",
       "       'Fatigue', ',', 'Long', 'Lasting', 'Cycles', '.', 'It', \"'s\",\n",
       "       'only', 'been', '5', '1/2', 'months', ',', 'but', 'i', \"'m\",\n",
       "       'concidering', 'changing', 'to', 'a', 'different', 'bc', '.',\n",
       "       'This', 'is', 'my', 'first', 'time', 'using', 'any', 'kind', 'of',\n",
       "       'bc', ',', 'unfortunately', 'due', 'to', 'the', 'constant',\n",
       "       'hassel', ',', 'i', \"'m\", 'not', 'happy', 'with', 'the', 'results',\n",
       "       '.'], dtype='<U13')"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_list_to_string = [''.join(i) for i in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = l.transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heavy'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "We construct two models for text classification. We are using the well known k-NN algorithm and AdaBoost?\n",
    "\n",
    "Our hypothesis is, that we can classify based on the review comment about the side effects, the gravity of such effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotSparse():\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self, X, y = None):\n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1st. Creating a training / test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the labels\n",
    "y_label = data['sideEffects']\n",
    "\n",
    "#encode the string values into numeric values\n",
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bar() missing 1 required positional argument: 'height'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-87ed8925cceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Histogram\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Side Effects'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Frequency'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bar() missing 1 required positional argument: 'height'"
     ]
    }
   ],
   "source": [
    "plt.bar(y, bins = 5, rwidth = 0.5)\n",
    "plt.title(\"Histogram\")\n",
    "plt.xlabel('Side Effects')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the data\n",
    "X = data['sideEffectsReview'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd. Creating a pipeline:\n",
    "\n",
    "The following workflow shows how the pipeline works.\n",
    "1. remove punctuations\n",
    "2. remove stopwords\n",
    "\n",
    "> After we removed punctuations and stopwords each document is a list of words. In the next step we have to merge the single documents into one corpus to apply transform it into a vector space model\n",
    "\n",
    "3. lower capitalized letters (We do this to avoid that the same word, where just the capitalized letter is the difference, is counted differently\n",
    "4. create a TF-ID matrix\n",
    "5. transfrom a sparse matrix into a numpy matrix (the TFID matrix is very sparse. To avoid memory problems the result is saved in a sparse format, this unfortunatelly does not allows directly to apply a ml algorithm. To avoid this we transform it into a numpy array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipeline = Pipeline([(\"Remove_Punktuation\", CleanText()),\n",
    "                         (\"Stopwords\", Stop('english')),\n",
    "                         (\"Lower\", Lower()),\n",
    "                          (\"TFID\", TF_ID(explainable = False)),\n",
    "                         (\"Matrix\", NotSparse())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3rd. apply kNN Classifier to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_kNN = Pipeline([(\"base\", base_pipeline),\n",
    "                        (\"kNN\", KNeighborsClassifier(algorithm = 'brute'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('base',\n",
       "                 Pipeline(memory=None,\n",
       "                          steps=[('Remove_Punktuation',\n",
       "                                  <__main__.CleanText object at 0x7feccf9f1c90>),\n",
       "                                 ('Stopwords',\n",
       "                                  <__main__.Stop object at 0x7feccf9f1fd0>),\n",
       "                                 ('Lower',\n",
       "                                  <__main__.Lower object at 0x7feccf9f1f50>),\n",
       "                                 ('TFID',\n",
       "                                  <__main__.TF_ID object at 0x7feccf9f1250>)],\n",
       "                          verbose=False)),\n",
       "                ('kNN',\n",
       "                 KNeighborsClassifier(algorithm='brute', leaf_size=30,\n",
       "                                      metric='minkowski', metric_params=None,\n",
       "                                      n_jobs=None, n_neighbors=5, p=2,\n",
       "                                      weights='uniform'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_kNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline_kNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 3, 3, 1, 1, 3, 3, 3, 1, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3,\n",
       "       2, 1, 1, 3, 2, 3, 2, 1, 3, 1, 3, 1, 3, 3, 2, 1, 1, 2, 3, 3, 3, 3,\n",
       "       3, 3, 2, 2, 1, 2, 3, 3, 2, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 2, 1,\n",
       "       3, 3, 1, 3, 2, 3, 3, 3, 1, 1, 3, 1, 1, 2, 1, 2, 3, 3, 1, 3, 3, 3,\n",
       "       3, 3, 3, 1, 2, 3, 2, 1, 2, 1, 1, 2, 3, 3, 2, 2, 3, 3, 2, 1, 1, 3,\n",
       "       2, 2, 3, 3, 1, 2, 1, 2, 3, 3, 1, 1, 2, 1, 3, 2, 1, 1, 3, 3, 1, 3,\n",
       "       3, 3, 3, 3, 3, 2, 2, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 2,\n",
       "       2, 3, 3, 1, 3, 1, 3, 2, 3, 2, 3, 3, 2, 2, 2, 1, 1, 3, 1, 1, 3, 3,\n",
       "       1, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 1, 2, 1, 1, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 1, 2, 2, 3, 3, 2, 2, 2, 3, 2, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3,\n",
       "       3, 3, 2, 3, 2, 1, 3, 2, 3, 1, 3, 3, 1, 3, 1, 2, 3, 2, 3, 3, 3, 3,\n",
       "       2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 2, 1, 2,\n",
       "       3, 1, 3, 3, 3, 2, 3, 3, 2, 3, 3, 1, 2, 1, 1, 2, 2, 2, 1, 3, 3, 3,\n",
       "       3, 3, 3, 1, 2, 1, 3, 3, 4, 3, 3, 1, 2, 3, 3, 2, 1, 3, 2, 2, 3, 3,\n",
       "       3, 3, 1])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40836012861736337"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_kNN.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_RF = Pipeline([(\"base\", base_pipeline),\n",
    "                        (\"RandomForest\", RandomForestClassifier(n_estimators = 10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('base',\n",
       "                 Pipeline(memory=None,\n",
       "                          steps=[('Remove_Punktuation',\n",
       "                                  <__main__.CleanText object at 0x7fecd5722ed0>),\n",
       "                                 ('Stopwords',\n",
       "                                  <__main__.Stop object at 0x7fecd57227d0>),\n",
       "                                 ('Lower',\n",
       "                                  <__main__.Lower object at 0x7fecd57229d0>),\n",
       "                                 ('TFID',\n",
       "                                  <__main__.TF_ID object at 0x7fecd5722a10>)],\n",
       "                          verbose=False)),\n",
       "                ('RandomForest',\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=None, max_features='auto',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=10, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5144694533762058"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_RF.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_kMean = Pipeline([(\"base\", base_pipeline),\n",
    "                        (\"k-Means\", KMeans(n_clusters = 5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('base',\n",
       "                 Pipeline(memory=None,\n",
       "                          steps=[('Remove_Punktuation',\n",
       "                                  <__main__.CleanText object at 0x7f837cdf9450>),\n",
       "                                 ('Stopwords',\n",
       "                                  <__main__.Stop object at 0x7f837cdf9bd0>),\n",
       "                                 ('Lower',\n",
       "                                  <__main__.Lower object at 0x7f837cdf9e10>),\n",
       "                                 ('TFID',\n",
       "                                  <__main__.TF_ID object at 0x7f837cdf9850>)],\n",
       "                          verbose=False)),\n",
       "                ('k-Means',\n",
       "                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\n",
       "                        max_iter=300, n_clusters=5, n_init=10, n_jobs=None,\n",
       "                        precompute_distances='auto', random_state=None,\n",
       "                        tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_kMean.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cluster_pred = pipeline_kMean.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 2, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cluster_pred[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically the TF-ID matrix is very sparse. This means most of the elements in the matrix are actual zeros. On the other hand we also have a high dimensionality. Especially algorithms which are based on distance measuring as the K-NN have troubles to compute the distance in several dimensions. This problem is known as \"curse of dimensionality\". Therefore we apply the principal component analysis method on our data to reduce the dimensionality without loosing to much of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotSparse():\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self, X, y = None):\n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_base2 = Pipeline([(\"base\", base_pipeline),\n",
    "                         (\"Converter\", NotSparse())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('base',\n",
       "                 Pipeline(memory=None,\n",
       "                          steps=[('Remove_Punktuation',\n",
       "                                  <__main__.CleanText object at 0x7feccf9f1c90>),\n",
       "                                 ('Stopwords',\n",
       "                                  <__main__.Stop object at 0x7feccf9f1fd0>),\n",
       "                                 ('Lower',\n",
       "                                  <__main__.Lower object at 0x7feccf9f1f50>),\n",
       "                                 ('TFID',\n",
       "                                  <__main__.TF_ID object at 0x7feccf9f1250>)],\n",
       "                          verbose=False)),\n",
       "                ('Converter', <__main__.NotSparse object at 0x7fecd5179910>)],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_base2.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lda = pipeline_base2.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3107, 5)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_X = lda.fit_transform(X_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cough, hypotension , proteinuria, impotence , renal failure , angina pectoris , tachycardia , eosinophilic pneumonitis, tastes disturbances , anusease anorecia , weakness fatigue insominca weakness'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37429717, 0.32348255, 0.25830738, ..., 0.20003413, 0.20682502,\n",
       "        0.20000873],\n",
       "       [0.20002131, 0.2000165 , 0.51165644, ..., 0.51095742, 0.20002475,\n",
       "        0.2007486 ],\n",
       "       [0.20002225, 0.20001694, 1.27249151, ..., 0.20005047, 0.76770701,\n",
       "        0.20001188],\n",
       "       [0.20002215, 0.20001764, 0.21004712, ..., 0.39927703, 0.20002777,\n",
       "        1.33643811],\n",
       "       [0.20002152, 0.20001768, 0.20008651, ..., 0.4898156 , 0.20002923,\n",
       "        0.20001237]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_terms=lda.components_\n",
    "topic_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-44ca186739fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtop_terms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;31m# number of 'top terms'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtopic_key_terms_idxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_terms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtopic_keyterms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_key_terms_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic_keyterms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_colwidth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'names' is not defined"
     ]
    }
   ],
   "source": [
    "# Extraqcting the most important 10 terms for each topic\n",
    "topic_terms=lda.components_\n",
    "top_terms=10 # number of 'top terms'\n",
    "topic_key_terms_idxs=np.argsort(-np.absolute(topic_terms), axis=1)[:,:top_terms]\n",
    "topic_keyterms=names[topic_key_terms_idxs]\n",
    "topics=[', '.join(topic) for topic in topic_keyterms]\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "topics_df=pd.DataFrame(topics,columns=['Term per Topic'], index=['Topic'+str(t) for t in range(1,5+1)])\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform document summarization we are using the genesis chapter from the bible. The dataset was downloaded from Kaggle and can be found under the following link: https://www.kaggle.com/nltkdata/genesis.\n",
    "Especially we use the english web file. The alternative would be to use the King James version of the bible. But here we have to take into account that special characters are included like \";\". Another thing is that the sentences are written in an older style which can cause troubles if we use models which use data trained on modern english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible = open(\"/Users/joel/Downloads/genesis/english-web.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the beginning God created the heavens and the earth.\n",
      "Now the earth was formless and empty.  Darkness was on the surface\n",
      "of the deep.  God's Spirit was hovering over the surface\n",
      "of the waters.\n",
      "God said, \"Let there be light,\" and there was light.\n",
      "God saw the light, and saw that it was good.  God divided\n",
      "the light from the darkness.\n",
      "God called the light Day, and the darkness he called Night.\n",
      "There was evening and there was morning, one day.\n",
      "God said, \"Let there be an expanse in the middle of the waters,\n",
      "and let it divide the waters from the waters.\"\n",
      "God made the expanse, and divided the waters which were under\n",
      "the expanse from the waters which were above the expanse;\n",
      "and it was so.\n",
      "God called the expanse sky.  There was evening and there\n",
      "was morning, a second day.\n",
      "God said, \"Let the waters under the sky be gathered together\n",
      "to one place, and let the dry land appear;\" and it was so.\n",
      "God called the dry land Earth, and the gathering together\n",
      "of the waters he called Seas.  God saw that it was good.\n",
      "God said, \"Let the earth put forth grass, herbs yielding seed,\n",
      "and fruit trees bearing fruit after their kind, with its seed\n",
      "in it, on the earth;\" and it was so.\n",
      "The earth brought forth grass, herbs yielding seed after their kind,\n",
      "and trees bearing fruit, with its seed in it, after their kind;\n",
      "and God saw that it was good.\n",
      "There was evening and there was morning, a third day.\n",
      "God said, \"Let there be lights in the expanse of sky to\n",
      "divide the day from the night; and let them be for signs,\n",
      "and for seasons, and for days and years;\n",
      "and let them be for lights in the expanse of sky to give light\n",
      "on the earth;\" and it was so.\n",
      "God made the two great lights:  the greater light to rule\n",
      "the day, and the lesser light to rule the night.\n",
      "He also made the stars.\n",
      "God set them in the expanse of sky to give light to the earth,\n",
      "and to rule over the day and over the night, and to divide\n",
      "the light from the darkness.  God saw that it was good.\n",
      "There was evening and there was morning, a fourth day.\n",
      "God said, \"Let the waters swarm with swarms of living creatures,\n",
      "and let birds fly above the earth in the open expanse of sky.\"\n",
      "God created the large sea creatures, and every living\n",
      "creature that moves, with which the waters swarmed,\n",
      "after their kind, and every winged bird after its kind.\n",
      "God saw that it was good.\n",
      "God blessed them, saying, \"Be fruitful, and multiply, and fill\n",
      "the waters in the seas, and let birds multiply on the earth.\"\n",
      "There was evening and there was morning, a fifth day.\n",
      "God said, \"Let the earth bring forth living creatures after\n",
      "their kind, livestock, creeping things, and animals of the earth\n",
      "after their kind;\" and it was so.\n",
      "God made the animals of the earth after their kind,\n",
      "and the livestock after their kind, and everything that creeps\n",
      "on the ground after its kind.  God saw that it was good.\n",
      "God said, \"Let us make man in our image, after our likeness:\n",
      "and let them have dominion over the fish of the sea,\n",
      "and over the birds of the sky, and over the livestock,\n",
      "and over all the earth, and over every creeping thing that\n",
      "creeps on the earth.\"\n",
      "God created man in his own image.  In God's image he created him;\n",
      "male and female he created them.\n",
      "God blessed them.  God said to them, \"Be fruitful, multiply,\n",
      "fill the earth, and subdue it.  Have dominion over the fish\n",
      "of the sea, over the birds of the sky, and over every living\n",
      "thing that moves on the earth.\"\n",
      "God said, \"Behold, I have given you every herb yielding seed,\n",
      "which is on the surface of all the earth, and every tree,\n",
      "which bears fruit yielding seed.  It will be your food.\n",
      "To every animal of the earth, and to every bird of the sky,\n",
      "and to everything that creeps on the earth, in which there is life,\n",
      "I have given every green herb for food;\" and it was so.\n",
      "God saw everything that he had made, and, behold, it was very good.\n",
      "There was evening and there was morning, a sixth day.\n",
      "The heavens and the earth were finished, and all their vast array.\n",
      "On the seventh day God finished his work which he had made;\n",
      "and he rested on the seventh day from all his work which\n",
      "he had made.\n",
      "God blessed the seventh day, and made it holy, because he rested\n",
      "in it from all his work which he had created and made.\n",
      "This is the history of the generations of the heavens and of\n",
      "the earth when they were created, in the day that Yahweh God\n",
      "made the earth and the heavens.\n",
      "No plant of the field was yet in the earth, and no herb of\n",
      "the field had yet sprung up; for Yahweh God had not caused it\n",
      "to rain on the earth.  There was not a man to till the ground,\n",
      "but a mist went up from the earth, and watered the whole\n",
      "surface of the ground.\n",
      "Yahweh God formed man from the dust of the ground,\n",
      "and breathed into his nostrils the breath of life; and man\n",
      "became a living soul.\n",
      "Yahweh God planted a garden eastward, in Eden, and there he put\n",
      "the man whom he had formed.\n",
      "Out of the ground Yahweh God made every tree to grow that is\n",
      "pleasant to the sight, and good for food; the tree of life\n",
      "also in the middle of the garden, and the tree of the knowledge\n",
      "of good and evil.\n",
      "A river went out of Eden to water the garden; and from there\n",
      "it was parted, and became four heads.\n",
      "The name of the first is Pishon:  this is the one which flows\n",
      "through the whole land of Havilah, where there is gold;\n",
      "and the gold of that land is good.  There is aromatic resin\n",
      "and the onyx stone.\n",
      "The name of the second river is Gihon:  the same river that flows\n",
      "through the whole land of Cush.\n",
      "The name of the third river is Hiddekel:  this is the one which\n",
      "flows in front of Assyria.  The fourth river is the Euphrates.\n",
      "Yahweh God took the man, and put him into the garden of Eden\n",
      "to dress it and to keep it.\n",
      "Yahweh God commanded the man, saying, \"Of every tree of the garden\n",
      "you may freely eat;\n",
      "but of the tree of the knowledge of good and evil, you shall\n",
      "not eat of it; for in the day that you eat of it you\n",
      "will surely die.\"\n",
      "Yahweh God said, \"It is not good that the man should be alone;\n",
      "I will make him a helper suitable for him.\"\n",
      "Out of the ground Yahweh God formed every animal of the field,\n",
      "and every bird of the sky, and brought them to the man to see\n",
      "what he would call them.  Whatever the man called every\n",
      "living creature, that was its name.\n",
      "The man gave names to all livestock, and to the birds of the sky,\n",
      "and to every animal of the field; but for man there was not\n",
      "found a helper suitable for him.\n",
      "Yahweh God caused a deep sleep to fall on the man, and he slept;\n",
      "and he took one of his ribs, and closed up the flesh\n",
      "in its place.\n",
      "He made the rib, which Yahweh God had taken from the man,\n",
      "into a woman, and brought her to the man.\n",
      "The man said, \"This is now bone of my bones, and flesh of my flesh.\n",
      "She will be called Woman, because she was taken out of Man.\"\n",
      "Therefore a man will leave his father and his mother,\n",
      "and will join with his wife, and they will be one flesh.\n",
      "They were both naked, the man and his wife, and were not ashamed.\n",
      "Now the serpent was more subtle than any animal of the field\n",
      "which Yahweh God had made.  He said to the woman, \"Has God\n",
      "really said, 'You shall not eat of any tree of the garden?'\"\n",
      "The woman said to the serpent, \"Of the fruit of the trees\n",
      "of the garden we may eat,\n",
      "but of the fruit of the tree which is in the middle of the garden,\n",
      "God has said, 'You shall not eat of it, neither shall you\n",
      "touch it, lest you die.'\"\n",
      "The serpent said to the woman, \"You won't surely die,\n",
      "for God knows that in the day you eat it, your eyes will be opened,\n",
      "and you will be like God, knowing good and evil.\"\n",
      "When the woman saw that the \n"
     ]
    }
   ],
   "source": [
    "print(bible.read(7500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195315"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bible.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_corpus = bible.read(7500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see the first 200 characters of the genesis chapter. As we see the chapter contains many characters. We will now use TextRank, a variation of the PageRank algorithm, to extract keywords out of the chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank4Sentences():\n",
    "    def __init__(self):\n",
    "        self.damping = 0.85  # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5  # convergence threshold\n",
    "        self.steps = 100  # iteration steps\n",
    "        self.text_str = None\n",
    "        self.sentences = None\n",
    "        self.pr_vector = None\n",
    "\n",
    "    def _sentence_similarity(self, sent1, sent2, stopwords=None):\n",
    "        if stopwords is None:\n",
    "            stopwords = []\n",
    "\n",
    "        sent1 = [w.lower() for w in sent1]\n",
    "        sent2 = [w.lower() for w in sent2]\n",
    "\n",
    "        all_words = list(set(sent1 + sent2))\n",
    "\n",
    "        vector1 = [0] * len(all_words)\n",
    "        vector2 = [0] * len(all_words)\n",
    "\n",
    "        # build the vector for the first sentence\n",
    "        for w in sent1:\n",
    "            if w in stopwords:\n",
    "                continue\n",
    "            vector1[all_words.index(w)] += 1\n",
    "\n",
    "        # build the vector for the second sentence\n",
    "        for w in sent2:\n",
    "            if w in stopwords:\n",
    "                continue\n",
    "            vector2[all_words.index(w)] += 1\n",
    "\n",
    "        return core_cosine_similarity(vector1, vector2)\n",
    "\n",
    "    def _build_similarity_matrix(self, sentences, stopwords=None):\n",
    "        # create an empty similarity matrix\n",
    "        sm = np.zeros([len(sentences), len(sentences)])\n",
    "\n",
    "        for idx1 in range(len(sentences)):\n",
    "            for idx2 in range(len(sentences)):\n",
    "                if idx1 == idx2:\n",
    "                    continue\n",
    "\n",
    "                sm[idx1][idx2] = self._sentence_similarity(sentences[idx1], sentences[idx2], stopwords=stopwords)\n",
    "\n",
    "        # Get Symmeric matrix\n",
    "        sm = get_symmetric_matrix(sm)\n",
    "\n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(sm, axis=0)\n",
    "        sm_norm = np.divide(sm, norm, where=norm != 0)  # this is ignore the 0 element in norm\n",
    "\n",
    "        return sm_norm\n",
    "\n",
    "    def _run_page_rank(self, similarity_matrix):\n",
    "\n",
    "        pr_vector = np.array([1] * len(similarity_matrix))\n",
    "\n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr_vector = (1 - self.damping) + self.damping * np.matmul(similarity_matrix, pr_vector)\n",
    "            if abs(previous_pr - sum(pr_vector)) < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr_vector)\n",
    "\n",
    "        return pr_vector\n",
    "\n",
    "    def _get_sentence(self, index):\n",
    "\n",
    "        try:\n",
    "            return self.sentences[index]\n",
    "        except IndexError:\n",
    "            return \"\"\n",
    "\n",
    "    def get_top_sentences(self, number=5):\n",
    "\n",
    "        top_sentences = []\n",
    "\n",
    "        if self.pr_vector is not None:\n",
    "\n",
    "            sorted_pr = np.argsort(self.pr_vector)\n",
    "            sorted_pr = list(sorted_pr)\n",
    "            sorted_pr.reverse()\n",
    "\n",
    "            index = 0\n",
    "            for epoch in range(number):\n",
    "                sent = self.sentences[sorted_pr[index]]\n",
    "                sent = normalize_whitespace(sent)\n",
    "                top_sentences.append(sent)\n",
    "                index += 1\n",
    "\n",
    "        return top_sentences\n",
    "\n",
    "    def analyze(self, text, stop_words=None):\n",
    "        self.text_str = text\n",
    "        self.sentences = sent_tokenize(self.text_str)\n",
    "\n",
    "        tokenized_sentences = [word_tokenize(sent) for sent in self.sentences]\n",
    "\n",
    "        similarity_matrix = self._build_similarity_matrix(tokenized_sentences, stop_words)\n",
    "\n",
    "        self.pr_vector = self._run_page_rank(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_TextRank = Pipeline([(\"Remove_Punktuation\", CleanText()),\n",
    "                 (\"Stopwords\", Stop('english')),\n",
    "                 (\"Lower\", Lower())])\n",
    "                #(\"Flatten\", Flatten())])\n",
    "                # (\"BoW\", CountVectorizer()),\n",
    "                 #(\"NotASparceMatrix\", NotSparse())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bible = pipe_TextRank.fit_transform(bible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bible = list(bible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in beginning god created heaven earth'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bible[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(set(X_bible[0] + X_bible[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector2 = [0] * len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = [0] * len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in X_bible[0]:\n",
    "\n",
    "    vector2[all_words.index(w)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in X_bible[1]:\n",
    "\n",
    "    vector1[all_words.index(w)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster.util import cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7787915579452677"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_cosine_similarity(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_cosine_similarity(vector1, vector2):\n",
    "    \"\"\"\n",
    "    measure cosine similarity between two vectors\n",
    "    :param vector1:\n",
    "    :param vector2:\n",
    "    :return: 0 < cosine similarity value < 1\n",
    "    \"\"\"\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "# example text\n",
    "text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\", last=True)\n",
    "doc = nlp(str(X_bible))\n",
    "\n",
    "# examine the top-ranked phrases in the document\n",
    "for p in doc._.phrases:\n",
    "    print(\"{:.4f} {:5d}  {}\".format(p.rank, p.count, p.text))\n",
    "    print(p.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the beginning God created the heavens and the earth.\n",
      "Now the earth was formless and empty.  Darkness was on the surface\n",
      "of the deep.  God's Spirit was hovering over the surface\n",
      "of the waters.\n",
      "God said, \"Let there be light,\" and there was light.\n",
      "God saw the light, and saw that it was good.  God divided\n",
      "the light from the darkness.\n",
      "God called the light Day, and the darkness he called Night.\n",
      "There was evening and there was morning, one day.\n",
      "God s\n"
     ]
    }
   ],
   "source": [
    "print(bible_corpus[0:450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the beginning God created the heavens and the earth.\n",
      "God called the light Day, and the darkness he called Night.\n",
      "God said, \"Let there be an expanse in the middle of the waters,\n",
      "God said, \"Let the waters under the sky be gathered together\n",
      "God called the dry land Earth, and the gathering together\n",
      "God said, \"Let the earth put forth grass, herbs yielding seed,\n",
      "The earth brought forth grass, herbs yielding seed after their kind,\n",
      "God said, \"Let there be lights in the expanse of sky to\n",
      "God set them in the expanse of sky to give light to the earth,\n",
      "God said, \"Let the waters swarm with swarms of living creatures,\n",
      "and let birds fly above the earth in the open expanse of sky.\"\n",
      "the waters in the seas, and let birds multiply on the earth.\"\n",
      "God said, \"Let the earth bring forth living creatures after\n",
      "their kind, livestock, creeping things, and animals of the earth\n",
      "God made the animals of the earth after their kind,\n",
      "God said, \"Let us make man in our image, after our likeness:\n",
      "God created man in his own image.\n",
      "God said, \"Behold, I have given you every herb yielding seed,\n",
      "To every animal of the earth, and to every bird of the sky,\n",
      "the earth when they were created, in the day that Yahweh God\n",
      "Yahweh God formed man from the dust of the ground,\n",
      "Yahweh God took the man, and put him into the garden of Eden\n",
      "Yahweh God commanded the man, saying, \"Of every tree of the garden\n",
      "Yahweh God said, \"It is not good that the man should be alone;\n",
      "Out of the ground Yahweh God formed every animal of the field,\n",
      "He made the rib, which Yahweh God had taken from the man,\n"
     ]
    }
   ],
   "source": [
    "print(summarize(bible_corpus, word_count=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We not used the complete genesis chapter. We focused on the first 7500 characters which belongs to the story of creating the earth, elements, animals humans, ... . The summarization works well. In the original corpus the tale starts with creating heaven and earth and the creation of night and day. To tell this a lot of fill words and sentences are used. For the same story genesis needed 9 sentences. Through summarization we could reduce the same story to two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bit746dcb02a1a346dfa7bf3b50e29c5539"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
